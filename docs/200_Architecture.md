# 200: Project Architecture

This document provides a detailed technical overview of the project's architecture. The system is a cloud-based data pipeline that ingests external articles, analyzes them against a curated knowledge base, and delivers intelligence via Microsoft Teams, a Web Portal, and Power BI.

## 1. Guiding Principles

The current design is guided by these principles:

* **Cloud-First:** The entire backend is hosted on an Azure VM to ensure 24/7 availability and resilience.
* **Modular Design:** The pipeline is broken into distinct, single-responsibility Python modules (Ingestion, Indexing, Analysis, Delivery).
* **Push & Pull Delivery:**
    *   **Push:** Automated daily "Morning Paper" digest sent to Teams for immediate awareness.
    *   **Pull:** Interactive Web Portal and Power BI dashboards for deep dives and historical analysis.

## 2. Key Components

The system is composed of three primary layers: Infrastructure, Data Pipeline, and Delivery.

### 2.1. Infrastructure

*   **Compute:** Azure Standard_B1s VM (1 vCPU, 1 GiB RAM, Ubuntu 22.04).
*   **Storage:**
    *   **Azure Database for PostgreSQL:** The single source of truth for all structured data.
    *   **Local Filesystem:** Used for ChromaDB (vector store) and temporary logs.
*   **Scheduling:** Cron is used to trigger the pipeline at specific times (7:00 AM and 3:00 PM ET).

### 2.2. Data Pipeline

The pipeline is a series of Python scripts executed sequentially by a bash wrapper (`run_pipeline.sh`) and a Python orchestrator (`test_orchestrator.py`).

1.  **Ingestion (`rss_fetcher.py`):** Fetches new articles from active feeds defined in the `sources` table. Checks against the database to prevent duplicates.
2.  **Indexing (`index_articles.py`):** Generates vector embeddings for new articles using `sentence-transformers` and stores them in ChromaDB.
3.  **Analysis (`analyze_articles.py`):** Calculates semantic similarity between new articles and user-defined "Semantic Anchors".
4.  **Enrichment (`enrich_articles.py`):** Applies the "Champion V4" logic (tiered thresholds) to flag high-relevance articles as `is_anchor_highlight` or `is_org_highlight`.
5.  **Data Export (`export_to_parquet.py`):** Exports processed data into optimized Parquet files for the Web Portal.

### 2.3. Delivery Layer

#### **The "Morning Paper" (Teams Digest)**
A custom Python module (`src.delivery`) generates a daily briefing.
*   **Engine:** Selects the top 3 articles per category based on priority flags and similarity scores.
*   **Renderer:** Formats the content into an Adaptive Card JSON.
*   **Transport:** Sends the card to a Microsoft Teams Webhook.

#### **Think Tank Intelligence Portal**
A static web application located in `portal/`.
*   **Technology:** Observable Framework + DuckDB-WASM.
*   **Data Source:** Consumes the Parquet files generated by the pipeline.
*   **Features:**
    *   **Morning Paper:** Client-side filtering of the last 7 days of intelligence.
    *   **Archive:** Fast, in-browser search of the entire 12,000+ article dataset.
*   **Deployment:** Designed to be hosted on GitHub Pages.

#### **Microsoft Power BI**
Power BI connects directly to the Azure PostgreSQL database to visualize the full dataset, enabling historical trend analysis and ad-hoc searching.

## 3. Data Model

*   **Source:** An organization or website being monitored.
*   **Article:** A single piece of external content (news, blog post).
*   **Semantic Anchor:** A user-defined topic of interest (e.g., "AI Regulation", "Housing Affordability").
*   **Similarity Score:** A numerical value (0-1) indicating how closely an article matches an anchor.
*   **Highlight Flag:** A boolean indicating if an article met the threshold to be featured.

## 4. Key Design Decisions

*   **Separation of Concerns:** Ingestion, Analysis, and Delivery are decoupled. This allows us to run the pipeline multiple times a day (ingestion) but only send notifications once (delivery).
*   **Adaptive Thresholds:** The system uses different relevance thresholds for different types of sources (e.g., stricter for general News Media, more lenient for specialized Think Tanks).
*   **Resilience:** The pipeline includes automatic retries and Teams alerting for failures, ensuring reliability without constant human monitoring.
*   **Local-First Frontend:** The web portal avoids complex server-side logic by using client-side DuckDB to query static Parquet files, keeping hosting simple and costs low.
